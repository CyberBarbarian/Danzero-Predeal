#!/usr/bin/env python3
"""
Quick test to validate the CPU-optimized configuration (150 workers √ó 4 envs = 600 envs).
This runs for 5 iterations to check stability and resource usage.
"""

import ray
import time
import sys
import os

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from guandan.rllib.trainers import create_dmc_trainer

print("="*80)
print("üß™ CPU OPTIMIZATION TEST - 200 Parallel Environments")
print("="*80)
print()

# Initialize Ray with test configuration
print("üîß Initializing Ray...")
ray.shutdown()
ray.init(
    ignore_reinit_error=True,
    num_gpus=2,
    num_cpus=64,  # Reduced from 192 to match fewer workers
    object_store_memory=10 * 1024 * 1024 * 1024,  # 10 GB (reduced from 30 GB)
)

print(f"‚úÖ Ray initialized")
print(f"   CPUs: 64")
print(f"   GPUs: 2")
print()

# Create trainer with new configuration
print("="*80)
print("üéØ Creating DMC Algorithm with CPU-optimized config...")
print("="*80)
print()
print("Configuration:")
print("  Workers: 50")
print("  Envs per worker: 4")
print("  Total environments: 200")
print("  GPU allocation: 1.0 for learner, 0.0 per worker")
print("  Batch size: 200 (optimized - now only computing Q for taken actions)")
print()

start_time = time.time()

algo = create_dmc_trainer(
    env_config={
        "observation_mode": "comprehensive",
        "use_internal_adapters": False,
        "max_steps": 1000,  # Safe limit - games naturally complete around 700 steps
    },
    num_workers=50,
    num_envs_per_worker=4,
    num_gpus=1.0,
    num_gpus_per_worker=0.0,
    lr=1e-3,
    batch_size=200,  # Match 200 parallel environments for maximum efficiency
    epsilon_start=0.2,
    epsilon_end=0.05,
    epsilon_decay_steps=10000,
)

init_time = time.time() - start_time
print(f"‚úÖ Algorithm initialized in {init_time:.2f}s")
print()

# Run test iterations
print("="*80)
print("üèÉ Running 100 Training Epochs with Monitoring...")
print("="*80)
print()

try:
    cumulative_episodes = 0
    for iteration in range(1, 101):
        iter_start = time.time()
        
        result = algo.train()
        
        iter_time = time.time() - iter_start
        
        # Extract metrics - RLlib now returns ResultsDict with Stats objects
        # Convert to simple dict for easier access
        try:
            result_dict = result.copy() if isinstance(result, dict) else {}
        except:
            result_dict = {}
        
        # Episodes - try to get from various sources
        episodes_this_iter = 0
        timesteps_total = 0
        episode_reward_mean = 0.0
        loss = 0.0
        
        # Try to extract basic training info
        try:
            # Get from __all_modules__ if available
            if '__all_modules__' in result_dict:
                all_modules = result_dict['__all_modules__']
                if hasattr(all_modules, 'peek'):
                    all_modules = all_modules.peek()
                if isinstance(all_modules, dict):
                    timesteps_total = all_modules.get('num_env_steps_trained', 0)
            
            # Get loss from first agent
            if 'agent_0' in result_dict:
                agent_0 = result_dict['agent_0']
                if hasattr(agent_0, 'peek'):
                    agent_0 = agent_0.peek()
                if isinstance(agent_0, dict):
                    loss_stat = agent_0.get('loss')
                    if loss_stat is not None:
                        loss = loss_stat.peek() if hasattr(loss_stat, 'peek') else loss_stat
            
            cumulative_episodes += episodes_this_iter
        except Exception as e:
            print(f"Warning: Could not extract all metrics: {e}")
        
        replay_size = 0  # Not tracked in new API
        
        print(f"[Iter {iteration}/100] "
              f"{iter_time:.2f}s | "
              f"Episodes: {cumulative_episodes:4d} (+{episodes_this_iter}) | "
              f"Steps: {timesteps_total:9d} | "
              f"Reward: {episode_reward_mean:7.2f} | "
              f"Loss: {loss:7.4f} | "
              f"Replay: {int(replay_size):5d}")
    
    print()
    print("="*80)
    print("‚úÖ TEST PASSED - Configuration is stable!")
    print("="*80)
    print()
    print("Key metrics:")
    print(f"  ‚Ä¢ Algorithm initialization: {init_time:.2f}s")
    print(f"  ‚Ä¢ Workers: 50")
    print(f"  ‚Ä¢ Total environments: 200")
    print(f"  ‚Ä¢ Total episodes collected: {cumulative_episodes}")
    print(f"  ‚Ä¢ Ready for production training")
    print()
    print("Note: Guandan is a zero-sum game, so average rewards across")
    print("      all agents will be ~0. Individual agent/team rewards vary.")
    print()
    print("Next step:")
    print("  bash scripts/training/train_production.sh")
    print("="*80)

except Exception as e:
    print()
    print("="*80)
    print(f"‚ùå TEST FAILED: {e}")
    print("="*80)
    import traceback
    traceback.print_exc()
    sys.exit(1)

finally:
    algo.stop()
    ray.shutdown()

