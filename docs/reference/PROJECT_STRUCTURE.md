# Project Structure - DanZero Guandan AI Framework

This repository contains a comprehensive reinforcement learning framework for the Guandan card game, evolved from the original DouZero Doudizhu codebase. The project focuses on building scalable distributed training systems and multiple baseline agent strategies.

## Overview

The project is structured around three main components:

- **Game Environment**: Complete Guandan game logic and state management
- **Agent System**: Multiple rule-based and learning-based AI strategies  
- **Training Framework**: Distributed reinforcement learning pipeline using Ray

## Directory Structure

### Root Level

```text
DanZero/
â”œâ”€â”€ train.py                    # Main training entry point
â”œâ”€â”€ setup.py                    # Package configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ requirements_lock.txt       # Locked dependency versions
â”œâ”€â”€ PROJECT_STRUCTURE.md        # This documentation
â”œâ”€â”€ README.md                   # Project overview and usage
â”œâ”€â”€ RLLIB_INTEGRATION_GUIDE.md  # RLLib integration documentation
â”œâ”€â”€ LICENSE                     # Apache 2.0 license
â”œâ”€â”€ actor.sh                    # Training script
â”œâ”€â”€ get_most_recent.sh          # Model retrieval script
â”œâ”€â”€ kill.sh                     # Process termination script
â”œâ”€â”€ logs/                        # Game log files directory
â”œâ”€â”€ most_recent_model/          # Model checkpoint directory
â”œâ”€â”€ Danvenv/                    # Python virtual environment
â”œâ”€â”€ archive/                    # Legacy Doudizhu code archive
â””â”€â”€ guandan/                    # Main package directory
```

### Main Package (`guandan/`)

#### Configuration

- **`config.py`** - Centralized training parameters and hyperparameters for DanZero framework

#### DeepMind Control Integration (`dmc/`)

```text
dmc/
â””â”€â”€ (empty)              # DeepMind Control integration (placeholder)
```

#### Game Environment (`env/`)

```text
env/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ game.py                    # Main game environment and self-play driver
â”œâ”€â”€ engine.py                  # Game rules, stage flow, and state transitions
â”œâ”€â”€ card_deck.py               # Two-deck card generation and dealing logic
â”œâ”€â”€ player.py                  # Player state and data structures
â”œâ”€â”€ context.py                 # Game context and shared state
â”œâ”€â”€ table.py                   # Table state management
â”œâ”€â”€ utils.py                   # Card pattern analysis and legal action generation
â”œâ”€â”€ move_detector.py           # Move validation and detection
â”œâ”€â”€ move_generator.py          # Legal move generation
â”œâ”€â”€ move_selector.py           # Move selection utilities
â”œâ”€â”€ observation_extractor.py   # JSON to numpy observation conversion (212-dim)
â””â”€â”€ rllib_env.py              # RLLib MultiAgentEnv wrapper (complete with agents)
```

#### Agent System (`agent/`)

```text
agent/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ agents.py            # Agent registry and factory
â”œâ”€â”€ random_agent.py      # Random baseline strategy
â”œâ”€â”€ ai1/                 # Complex rule-based strategy (migrated)
â”œâ”€â”€ ai2/                 # Phased decision-making strategy (migrated)
â”œâ”€â”€ ai3/                 # Experimental/adversarial strategy (migrated)
â”œâ”€â”€ ai4/                 # Alternative heuristic approach (migrated)
â”œâ”€â”€ ai6/                 # Additional heuristic strategy (migrated)
â”œâ”€â”€ mc/                  # Monte Carlo baseline (legacy)
â”œâ”€â”€ baselines/           # Rule-based AI strategies
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md        # Baseline strategies documentation
â”‚   â”œâ”€â”€ rule/            # Rule-based AI implementations
â”‚   â””â”€â”€ legacy/          # Archived baseline implementations
â”‚       â””â”€â”€ mc/          # Monte Carlo baseline (archived)
â””â”€â”€ torch/               # Neural network-based agents
    â”œâ”€â”€ actor.py         # Actor network implementation
    â”œâ”€â”€ client.py        # Torch agent client
    â”œâ”€â”€ model.py         # Neural network models
    â”œâ”€â”€ util.py          # Utility functions
    â”œâ”€â”€ ppo20000.pth     # Pre-trained PPO model
    â””â”€â”€ q_network.ckpt   # Pre-trained Q-network model
```

#### Training Framework (`training/`)

```text
training/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ ray_app.py           # Ray-based training orchestrator
â”œâ”€â”€ parameter_server.py  # Distributed parameter server
â”œâ”€â”€ rollout_worker.py    # Self-play data collection worker
â”œâ”€â”€ learner.py           # Model training and optimization
â”œâ”€â”€ checkpoint.py        # Model checkpoint management
â””â”€â”€ logger.py            # Training metrics and logging
```

### Archive (`archive/`)

```text
archive/
â””â”€â”€ doudizhu/            # Original DouZero Doudizhu implementation
    â”œâ”€â”€ README_doudizhu_original.md
    â”œâ”€â”€ evaluate.py
    â”œâ”€â”€ ADP_test.py
    â”œâ”€â”€ generate_eval_data.py
    â”œâ”€â”€ sl_test.py
    â”œâ”€â”€ test.py
    â””â”€â”€ most_recent_model/
        â”œâ”€â”€ landlord.ckpt
        â”œâ”€â”€ landlord_up.ckpt
        â””â”€â”€ landlord_down.ckpt
```

## Key Components

### Game Environment

- **Complete Guandan Implementation**: Four-player team-based card game with tribute/return mechanics
- **State Management**: Comprehensive game state tracking including hand cards, table state, and game phases
- **Action Space**: Legal move generation and validation for all card combinations
- **Rule Engine**: Stage-based game flow with level progression and tribute mechanics

### Agent Strategies

- **Rule-based Baselines**: Multiple heuristic strategies (ai1-ai6) with different decision-making approaches
- **Random Agent**: Simple random baseline for sanity checking
- **Neural Network Agents**: Legacy PPO and Q-network implementations
- **Monte Carlo**: Archived Monte Carlo tree search baseline

### Training Framework

- **Ray Integration**: Distributed training pipeline for scalable self-play
- **Parameter Server**: Centralized model parameter management
- **Rollout Workers**: Parallel data collection from self-play games
- **Learner**: Model training and optimization with experience replay

## Dependencies

### Core Dependencies

- **PyTorch**: Deep learning framework for neural network agents
- **Ray**: Distributed computing framework for training
- **RLCard**: Card game environment utilities
- **GitPython**: Version control integration

### Development Dependencies

- **Dill**: Advanced serialization for Python objects
- **Lightning Utilities**: PyTorch Lightning utilities
- **Opt-Einsum**: Optimized Einstein summation operations

## Architecture Overview

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Training      â”‚    â”‚   Game           â”‚    â”‚   Agent         â”‚
â”‚   Framework     â”‚    â”‚   Environment    â”‚    â”‚   System        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Ray App       â”‚â—„â”€â”€â–ºâ”‚ â€¢ Game Engine    â”‚â—„â”€â”€â–ºâ”‚ â€¢ Rule Agents   â”‚
â”‚ â€¢ Parameter     â”‚    â”‚ â€¢ State Mgmt     â”‚    â”‚ â€¢ Neural Agents â”‚
â”‚   Server        â”‚    â”‚ â€¢ Action Space   â”‚    â”‚ â€¢ Random Agent  â”‚
â”‚ â€¢ Rollout       â”‚    â”‚ â€¢ Move Detection â”‚    â”‚ â€¢ Agent Registryâ”‚
â”‚   Workers       â”‚    â”‚ â€¢ Card Deck      â”‚    â”‚                 â”‚
â”‚ â€¢ Learner       â”‚    â”‚ â€¢ Player Context â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚   RLLib          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Integration    â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚ â€¢ MultiAgentEnv  â”‚
                        â”‚ â€¢ Observation    â”‚
                        â”‚   Extractor      â”‚
                        â”‚ â€¢ Test Suites    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development Status

### Completed

- âœ… Guandan game environment implementation
- âœ… Rule-based agent strategies (ai1-ai6)
- âœ… Agent registry and factory system
- âœ… RLLib MultiAgentEnv integration
- âœ… Paper-compliant observation space (513-dimensional)
- âœ… Win/loss logic integration in game core
- âœ… Tribute system with proper limits (10 points max)
- âœ… Rank progression and upgrade mechanics
- âœ… Tournament system with proper win detection

### In Progress

- ğŸ”„ BaseAgent interface unification
- ğŸ”„ Training framework implementation
- ğŸ”„ Performance optimization and testing

### Planned

- ğŸ“‹ Complete training pipeline
- ğŸ“‹ Model evaluation and benchmarking
- ğŸ“‹ Advanced training strategies

## Usage

### Quick Start

#### Traditional Agent Usage

```python
from guandan.agent.agents import agent_cls

# Create a rule-based agent
agent = agent_cls['ai1'](id=0)

# Process game message and get action
action_index = agent.received_message(game_message)
```

#### RLLib Environment Usage

```python
from guandan.env.rllib_env import GuandanMultiAgentEnv

# Create environment
env = GuandanMultiAgentEnv()

# Reset and get observations
obs = env.reset()
print(f"Observation space: {env.observation_space}")
print(f"Action space: {env.action_space}")

# Step through environment
action = env.action_space.sample()  # Random action
obs, rewards, dones, infos = env.step(action)
```

### Training

```bash
# Start distributed training
python train.py --xpid danzero_experiment --total_frames 10000000
```

### Testing

```bash
# Test RLLib environment
python -c "from guandan.env.rllib_env import make_guandan_env; env = make_guandan_env(); print('Environment ready!')"

# View game logs
ls logs/
```

## Contributing

This project welcomes contributions in:

- Guandan rule implementations and edge cases
- New agent strategies and algorithms
- Training pipeline optimizations
- Performance optimizations
- Documentation and examples

## License

Apache License 2.0 - see LICENSE file for details.
