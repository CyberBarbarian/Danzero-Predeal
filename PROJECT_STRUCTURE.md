# Project Structure - DanZero Guandan AI Framework

This repository contains a comprehensive reinforcement learning framework for the Guandan card game, evolved from the original DouZero Doudizhu codebase. The project focuses on building scalable distributed training systems and multiple baseline agent strategies.

## Overview

The project is structured around three main components:

- **Game Environment**: Complete Guandan game logic and state management
- **Agent System**: Multiple rule-based and learning-based AI strategies  
- **Training Framework**: Distributed reinforcement learning pipeline using Ray

## Directory Structure

### Root Level

```text
guandan_douzero/
â”œâ”€â”€ train.py                    # Main training entry point
â”œâ”€â”€ setup.py                    # Package configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ requirements_lock.txt       # Locked dependency versions
â”œâ”€â”€ PROJECT_STRUCTURE.md        # This documentation
â”œâ”€â”€ README.md                   # Project overview and usage
â”œâ”€â”€ LICENSE                     # Apache 2.0 license
â”œâ”€â”€ actor.sh                    # Training script
â”œâ”€â”€ get_most_recent.sh          # Model retrieval script
â”œâ”€â”€ kill.sh                     # Process termination script
â”œâ”€â”€ Danvenv/                    # Python virtual environment
â”œâ”€â”€ archive/                    # Legacy Doudizhu code archive
â””â”€â”€ guandan/                    # Main package directory
```

### Main Package (`guandan/`)

#### Configuration

- **`config.py`** - Centralized training parameters and hyperparameters for DanZero framework

#### Game Environment (`env/`)

```text
env/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ game.py              # Main game environment and self-play driver
â”œâ”€â”€ engine.py            # Game rules, stage flow, and state transitions
â”œâ”€â”€ card_deck.py         # Two-deck card generation and dealing logic
â”œâ”€â”€ player.py            # Player state and data structures
â”œâ”€â”€ context.py           # Game context and shared state
â”œâ”€â”€ table.py             # Table state management
â”œâ”€â”€ utils.py             # Card pattern analysis and legal action generation
â”œâ”€â”€ move_detector.py     # Move validation and detection
â”œâ”€â”€ move_generator.py    # Legal move generation
â””â”€â”€ move_selector.py     # Move selection utilities
```

#### Agent System (`agent/`)

```text
agent/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ agents.py            # Agent registry and factory
â”œâ”€â”€ random_agent.py      # Random baseline strategy
â”œâ”€â”€ baselines/           # Rule-based AI strategies
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md        # Baseline strategies documentation
â”‚   â”œâ”€â”€ rule/            # Rule-based AI implementations
â”‚   â”‚   â”œâ”€â”€ ai1/         # Complex rule-based strategy
â”‚   â”‚   â”œâ”€â”€ ai2/         # Phased decision-making strategy
â”‚   â”‚   â”œâ”€â”€ ai3/         # Experimental/adversarial strategy
â”‚   â”‚   â”œâ”€â”€ ai4/         # Alternative heuristic approach
â”‚   â”‚   â””â”€â”€ ai6/         # Additional heuristic strategy
â”‚   â””â”€â”€ legacy/          # Archived baseline implementations
â”‚       â””â”€â”€ mc/          # Monte Carlo baseline (archived)
â””â”€â”€ torch/               # Neural network-based agents
    â”œâ”€â”€ actor.py         # Actor network implementation
    â”œâ”€â”€ client.py        # Torch agent client
    â”œâ”€â”€ model.py         # Neural network models
    â”œâ”€â”€ util.py          # Utility functions
    â”œâ”€â”€ ppo20000.pth     # Pre-trained PPO model
    â””â”€â”€ q_network.ckpt   # Pre-trained Q-network model
```

#### Training Framework (`training/`)

```text
training/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ ray_app.py           # Ray-based training orchestrator
â”œâ”€â”€ parameter_server.py  # Distributed parameter server
â”œâ”€â”€ rollout_worker.py    # Self-play data collection worker
â”œâ”€â”€ learner.py           # Model training and optimization
â”œâ”€â”€ checkpoint.py        # Model checkpoint management
â””â”€â”€ logger.py            # Training metrics and logging
```

### Archive (`archive/`)

```text
archive/
â””â”€â”€ doudizhu/            # Original DouZero Doudizhu implementation
    â”œâ”€â”€ README_doudizhu_original.md
    â”œâ”€â”€ evaluate.py
    â”œâ”€â”€ ADP_test.py
    â”œâ”€â”€ generate_eval_data.py
    â”œâ”€â”€ sl_test.py
    â”œâ”€â”€ test.py
    â””â”€â”€ most_recent_model/
        â”œâ”€â”€ landlord.ckpt
        â”œâ”€â”€ landlord_up.ckpt
        â””â”€â”€ landlord_down.ckpt
```

## Key Components

### Game Environment

- **Complete Guandan Implementation**: Four-player team-based card game with tribute/return mechanics
- **State Management**: Comprehensive game state tracking including hand cards, table state, and game phases
- **Action Space**: Legal move generation and validation for all card combinations
- **Rule Engine**: Stage-based game flow with level progression and tribute mechanics

### Agent Strategies

- **Rule-based Baselines**: Multiple heuristic strategies (ai1-ai6) with different decision-making approaches
- **Random Agent**: Simple random baseline for sanity checking
- **Neural Network Agents**: Legacy PPO and Q-network implementations
- **Monte Carlo**: Archived Monte Carlo tree search baseline

### Training Framework

- **Ray Integration**: Distributed training pipeline for scalable self-play
- **Parameter Server**: Centralized model parameter management
- **Rollout Workers**: Parallel data collection from self-play games
- **Learner**: Model training and optimization with experience replay

## Dependencies

### Core Dependencies

- **PyTorch**: Deep learning framework for neural network agents
- **Ray**: Distributed computing framework for training
- **RLCard**: Card game environment utilities
- **GitPython**: Version control integration

### Development Dependencies

- **Dill**: Advanced serialization for Python objects
- **Lightning Utilities**: PyTorch Lightning utilities
- **Opt-Einsum**: Optimized Einstein summation operations

## Architecture Overview

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Training      â”‚    â”‚   Game           â”‚    â”‚   Agent         â”‚
â”‚   Framework     â”‚    â”‚   Environment    â”‚    â”‚   System        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Ray App       â”‚â—„â”€â”€â–ºâ”‚ â€¢ Game Engine    â”‚â—„â”€â”€â–ºâ”‚ â€¢ Rule Agents   â”‚
â”‚ â€¢ Parameter     â”‚    â”‚ â€¢ State Mgmt     â”‚    â”‚ â€¢ Neural Agents â”‚
â”‚   Server        â”‚    â”‚ â€¢ Action Space   â”‚    â”‚ â€¢ Random Agent  â”‚
â”‚ â€¢ Rollout       â”‚    â”‚ â€¢ Move Detection â”‚    â”‚ â€¢ Agent Registryâ”‚
â”‚   Workers       â”‚    â”‚ â€¢ Card Deck      â”‚    â”‚                 â”‚
â”‚ â€¢ Learner       â”‚    â”‚ â€¢ Player Context â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development Status

### Completed

- âœ… Guandan game environment implementation
- âœ… Multiple rule-based agent strategies
- âœ… Basic training framework structure
- âœ… Agent registry and factory system
- âœ… Legacy code archival and organization

### In Progress

- ğŸ”„ Observation standardization and action encoding
- ğŸ”„ BaseAgent interface unification
- ğŸ”„ Ray training pipeline implementation
- ğŸ”„ Feature extraction and encoding modules

### Planned

- ğŸ“‹ Comprehensive testing suite
- ğŸ“‹ Performance optimization
- ğŸ“‹ Documentation and examples
- ğŸ“‹ Model evaluation and benchmarking

## Usage

### Quick Start

```python
from guandan.agent.agents import agent_cls

# Create a rule-based agent
agent = agent_cls['ai1'](id=0)

# Process game message and get action
action_index = agent.received_message(game_message)
```

### Training

```bash
# Start distributed training
python train.py --xpid danzero_experiment --total_frames 10000000
```

## Contributing

This project welcomes contributions in:

- Guandan rule implementations and edge cases
- New agent strategies and algorithms
- Training pipeline optimizations
- Testing and validation improvements
- Documentation and examples

## License

Apache License 2.0 - see LICENSE file for details.
